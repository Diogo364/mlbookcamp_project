{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 03\n",
    "---\n",
    "[mlbookcamp 03-classification](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/cohorts/2022/03-classification/homework.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRPATH=\"./data/\"\n",
    "FILENAME = \"housing.csv\"\n",
    "DATAPATH = osp.join(DIRPATH, FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data//housing.csv already exists.\n"
     ]
    }
   ],
   "source": [
    "! ./downloading_data.sh -d $DIRPATH -f $FILENAME https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATAPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "For the rest of the homework, you'll need to use the features from the previous homework with additional two 'neighbourhood_group' and 'room_type'. So the whole feature set will be set as follows:\n",
    "\n",
    "- 'latitude',\n",
    "- 'longitude',\n",
    "- 'housing_median_age',\n",
    "- 'total_rooms',\n",
    "- 'total_bedrooms',\n",
    "- 'population',\n",
    "- 'households',\n",
    "- 'median_income',\n",
    "- 'median_house_value',\n",
    "- 'ocean_proximity'\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "- Select only the features from above and fill in the missing values with 0.\n",
    "- Create a new column rooms_per_household by dividing the column total_rooms by the column households from dataframe.\n",
    "- Create a new column bedrooms_per_room by dividing the column total_bedrooms by the column total_rooms from dataframe.\n",
    "- Create a new column population_per_household by dividing the column population by the column households from dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = [\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'housing_median_age',\n",
    "    'total_rooms',\n",
    "    'total_bedrooms',\n",
    "    'population',\n",
    "    'households',\n",
    "    'median_income',\n",
    "    'median_house_value',\n",
    "    'ocean_proximity',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[required_columns].fillna(0)\n",
    "data['rooms_per_household'] = data['total_rooms'] / data['households']\n",
    "data['bedrooms_per_room'] = data['total_bedrooms'] / data['total_rooms']\n",
    "data['population_per_household'] = data['population'] / data['households']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "What is the most frequent observation (mode) for the column ocean_proximity?\n",
    "\n",
    "Options:\n",
    "\n",
    "- NEAR BAY\n",
    "- <1H OCEAN\n",
    "- INLAND\n",
    "- NEAR OCEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <1H OCEAN\n",
       "dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ocean_proximity'].mode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Create the correlation matrix for the numerical features of your train dataset.\n",
    "In a correlation matrix, you compute the correlation coefficient between every pair of features in the dataset.\n",
    "What are the two features that have the biggest correlation in this dataset?\n",
    "Options:\n",
    "\n",
    "- total_bedrooms and households\n",
    "- total_bedrooms and total_rooms\n",
    "- population and households\n",
    "- population_per_household and total_rooms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>households</td>\n",
       "      <td>total_bedrooms</td>\n",
       "      <td>0.966507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature_0       feature_1  correlation\n",
       "76  households  total_bedrooms     0.966507"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correlation_table = data.corr().stack().reset_index().rename({'level_0':'feature_0', 'level_1':'feature_1', 0:'correlation'}, axis=1)\n",
    "correlation_table.correlation = correlation_table.correlation.abs()\n",
    "correlation_table.loc[correlation_table.feature_0 != correlation_table.feature_1, :].sort_values(by='correlation', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make median_house_value binary\n",
    "- We need to turn the median_house_value variable from numeric into binary.\n",
    "- Let's create a variable above_average which is 1 if the median_house_value is above its mean value and 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['above_average'] = (data['median_house_value'] > data['median_house_value'].mean()).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data\n",
    "- Split your data in train/val/test sets, with 60%/20%/20% distribution.\n",
    "- Use Scikit-Learn for that (the train_test_split function) and set the seed to 42.\n",
    "- Make sure that the target value ('median_house_value') is not in your dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data.drop(['above_average', 'median_house_value'], axis=1), \n",
    "                                                    data['above_average'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, \n",
    "                                                Y_train, \n",
    "                                                test_size=0.25, \n",
    "                                                shuffle=False, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6, 0.2, 0.2])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray([X_train.shape[0], X_test.shape[0], X_val.shape[0]])/data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "- Calculate the mutual information score between above_average and ocean_proximity . Use the training set only.\n",
    "- Round it to 2 decimals using round(score, 2)\n",
    "- What is their mutual information score?\n",
    "\n",
    "Options:\n",
    "\n",
    "- 0.26\n",
    "- 0\n",
    "- 0.10\n",
    "- 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ocean_proximity']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_types = X_train.dtypes\n",
    "cat_features = list(column_types[column_types == 'object'].index)\n",
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ocean_proximity': 0.1}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "info_score_dict = {feature: np.round(mutual_info_score(X_train[feature], Y_train), 2) \n",
    "    for feature in cat_features}\n",
    "info_score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "- Now let's train a logistic regression\n",
    "- Remember that we have one categorical variable ocean_proximity in the data. Include it using one-hot encoding.\n",
    "- Fit the model on the training dataset.\n",
    "  - To make sure the results are reproducible across different versions of Scikit-Learn, fit the model with these parameters:\n",
    "  - model = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=1000, random_state=42)\n",
    "- Calculate the accuracy on the validation dataset and round it to 2 decimal digits.\n",
    "\n",
    "Options:\n",
    "\n",
    "- 0.60\n",
    "- 0.72\n",
    "- 0.84\n",
    "- 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "preprocessing = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(), cat_features)], \n",
    "    remainder='passthrough'\n",
    "    )\n",
    "X_train_processed = preprocessing.fit_transform(X_train)\n",
    "X_test_processed = preprocessing.transform(X_test)\n",
    "X_val_processed = preprocessing.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, random_state=42, solver='liblinear')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver=\"liblinear\", C=1.0, max_iter=1000, random_state=42)\n",
    "model.fit(X_train_processed, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "yhat_val = model.predict(X_val_processed)\n",
    "acc = accuracy_score(Y_val, yhat_val)\n",
    "np.round(acc, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "- Let's find the least useful feature using the feature elimination technique.\n",
    "- Train a model with all these features (using the same parameters as in Q4).\n",
    "- Now exclude each feature from this set and train a model without it. Record the accuracy for each model.\n",
    "- For each feature, calculate the difference between the original accuracy and the accuracy without the feature.\n",
    "- Which of following feature has the smallest difference?\n",
    "    -  total_rooms\n",
    "    -  total_bedrooms\n",
    "    -  population\n",
    "    -  households\n",
    "> note: the difference doesn't have to be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_rooms': 0.8289728682170543,\n",
       " 'total_bedrooms': 0.8263081395348837,\n",
       " 'population': 0.8391472868217055,\n",
       " 'households': 0.8301841085271318}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection_dict = {}\n",
    "for del_feature in ['total_rooms', 'total_bedrooms', 'population', 'households']:\n",
    "    selected_cat_features = [feature for feature in cat_features if feature != del_feature]\n",
    "    x_query = X_train.drop(columns=del_feature)\n",
    "    val_query = X_val.drop(columns=del_feature)\n",
    "    preprocessing = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(), selected_cat_features)], \n",
    "        remainder='passthrough'\n",
    "        )\n",
    "    x_query = preprocessing.fit_transform(x_query)\n",
    "    val_query = preprocessing.transform(val_query)\n",
    "    model.fit(x_query, Y_train)\n",
    "    feature_selection_dict[del_feature] = accuracy_score(Y_val, model.predict(val_query))\n",
    "feature_selection_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "total_rooms       0.000969\n",
       "total_bedrooms    0.001696\n",
       "households        0.002180\n",
       "population        0.011143\n",
       "Name: accuracy, dtype: float64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selection_df = pd.DataFrame(feature_selection_dict, index=['accuracy']).T\n",
    "(acc - feature_selection_df.accuracy).abs().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "- For this question, we'll see how to use a linear regression model from Scikit-Learn\n",
    "- We'll need to use the original column 'median_house_value'. Apply the logarithmic transformation to this column.\n",
    "- Fit the Ridge regression model (model = Ridge(alpha=a, solver=\"sag\", random_state=42)) on the training data.\n",
    "- This model has a parameter alpha. Let's try the following values: `[0, 0.01, 0.1, 1, 10]`\n",
    "- Which of these alphas leads to the best RMSE on the validation set? Round your RMSE scores to 3 decimal digits.\n",
    "\n",
    "If there are multiple options, select the smallest alpha.\n",
    "\n",
    "Options:\n",
    "\n",
    "- 0\n",
    "- 0.01\n",
    "- 0.1\n",
    "- 1\n",
    "- 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.log1p(data.median_house_value)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data.drop(['above_average', 'median_house_value'], axis=1), \n",
    "                                                    Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle=True, \n",
    "                                                    random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, \n",
    "                                                Y_train, \n",
    "                                                test_size=0.25, \n",
    "                                                shuffle=False, \n",
    "                                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "alpha_values = [0, 0.01, 0.1, 1, 10]\n",
    "score_dict = {'alpha': [], 'val_score': []}\n",
    "for alpha in alpha_values:\n",
    "    model = Ridge(alpha=alpha, solver=\"sag\", random_state=42)\n",
    "    model.fit(X_train_processed, Y_train)\n",
    "    yhat = model.predict(X_val_processed)\n",
    "    score_dict['alpha'].append(alpha)\n",
    "    score_dict['val_score'].append(mean_squared_error(Y_val, yhat, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>val_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.00</td>\n",
       "      <td>0.534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alpha  val_score\n",
       "0   0.00      0.534\n",
       "1   0.01      0.534\n",
       "2   0.10      0.534\n",
       "3   1.00      0.534\n",
       "4  10.00      0.534"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df = pd.DataFrame(score_dict)\n",
    "score_df['val_score'] = score_df['val_score'].round(3)\n",
    "score_df.sort_values(by=['val_score', 'alpha'], ascending=[False, True])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('general-conda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a6c412e415f27236cb9d9dc68868e41cb65d54679c06807ebc8d62a20f3611d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
